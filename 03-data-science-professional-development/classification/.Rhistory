install.packages("ggplot2")
install.packages("dplyr")
install.packages("tidyverse")
install.packages("mlr3")
install.packages("tibble")
install.packages("xml")
install.packages("XML")
install.packages("keras")
install.packages("deepnet")
install.packages("mxnet")
# 示例数据
data <- data.frame(
Feature = c('sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'overcast', 'rainy', 'rainy'),
Play = c('no', 'yes', 'yes', 'yes', 'yes', 'yes', 'no', 'no')
)
naive_bayes_train <- function(data, feature_col, class_col) {
# 先验概率
priors <- table(data[[class_col]]) / nrow(data)
# 条件概率
conditional_probs <- list()
for(class in unique(data[[class_col]])) {
subset_data <- data[data[[class_col]] == class, ]
conditional_probs[[class]] <- table(subset_data[[feature_col]]) / nrow(subset_data)
}
list(priors = priors, conditional_probs = conditional_probs)
}
naive_bayes_predict <- function(model, new_feature) {
scores <- c()
for(class in names(model$priors)) {
# P(class)
score <- model$priors[class]
# P(feature|class)
if(new_feature %in% names(model$conditional_probs[[class]])) {
score <- score * model$conditional_probs[[class]][new_feature]
} else {
score <- 0
}
scores[class] <- score
}
names(which.max(scores))
}
# 训练
model <- naive_bayes_train(data, 'Feature', 'Play')
# 预测
prediction <- naive_bayes_predict(model, 'sunny')
print(prediction)
packageVersion('gRain')
install.packages("devtools")
devtools::install_version("gRain", version = "1.3.13")
str(train_data)
# 导入必备的库
library(rpart)
library(rpart.plot)
library(caret)
library(randomForest)
# 导入数据集
melb_house_data <- read.csv("melb_house_withnum.csv", header = TRUE)
set("C:/Users/yejiu/Desktop/Github/master-of-data-science-unisa/03-DataScienceProfessionalDevelopment/classification")
setwd("C:/Users/yejiu/Desktop/Github/master-of-data-science-unisa/03-DataScienceProfessionalDevelopment/classification")
# 导入必备的库
library(rpart)
library(rpart.plot)
library(caret)
library(randomForest)
# 导入数据集
melb_house_data <- read.csv("melb_house_withnum.csv", header = TRUE)
# head(melb_house_data)
# 去除缺失值
melb_house_data_cleaned <- na.omit(melb_house_data)
# 去除一些列
melb_house_data_cleaned <- melb_house_data_cleaned[ , !(names(melb_house_data_cleaned) %in% c("ADDRESS", "SELLERG", "METHOD", "DATE", "COUNCILARE", "BUILDINGAR", "YEARBUILT", "LATTITUDE", "LONGTITUDE", "PROPERTYCO", "SUBURB"))]
# 计算IQR
Q1 <- quantile(melb_house_data_cleaned$PRICE, 0.25)
Q3 <- quantile(melb_house_data_cleaned$PRICE, 0.75)
IQR <- Q3 - Q1
# 定义上下界
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
# 根据上下界过滤数据
melb_house_data_cleaned <- melb_house_data_cleaned[melb_house_data_cleaned$PRICE >= lower_bound & melb_house_data_cleaned$PRICE <= upper_bound, ]
# 检测price的分布情况
# 将PRICE分成10w的区间
bins <- seq(0, max(melb_house_data_cleaned$PRICE) + 100000, by=100000)
price_intervals <- cut(melb_house_data_cleaned$PRICE, breaks=bins, right=FALSE)
# 计算每个区间的频数
price_counts <- table(price_intervals)
# 根据新的区间划分price_interval
breaks <- c(1e+05, 2e+05, 3e+05, 4e+05, 5e+05, 6e+05, 7e+05, 8e+05, 9e+05, 1e+06, 1.1e+06, 1.6e+06, 2.1e+06, 2.4e+06)
labels <- c("[1e+05, 2e+05)", "[2e+05, 3e+05)", "[3e+05, 4e+05)", "[4e+05, 5e+05)", "[5e+05, 6e+05)",
"[6e+05, 7e+05)", "[7e+05, 8e+05)", "[8e+05, 9e+05)", "[9e+05, 1e+06)", "[1e+06, 1.1e+06)",
"[1.1e+06, 1.6e+06)", "[1.6e+06, 2.1e+06)", "[2.1e+06, 2.4e+06)")
melb_house_data_cleaned$price_interval <- cut(melb_house_data_cleaned$PRICE, breaks = breaks, labels = labels, include.lowest = TRUE, right = FALSE)
# 定义5公里的区间断点
bins_distance <- seq(0, 50, by=5)  # 0到50公里，每5公里一个区间
# 使用cut()函数将DISTANCE转换为5公里的区间
distance_intervals <- cut(melb_house_data_cleaned$DISTANCE, breaks=bins_distance, right=FALSE)
# 将计算出的距离区间添加到数据框作为新列
melb_house_data_cleaned$distance_interval <- distance_intervals
# 设置随机数生成种子，以确保可复现的结果
set.seed(123)
# 计算训练数据的大小
train_size <- floor(0.7 * nrow(melb_house_data_cleaned))
# 随机抽取70%的索引作为训练数据
train_indices <- sample(seq_len(nrow(melb_house_data_cleaned)), size = train_size)
# 根据上述索引创建训练数据和测试数据
train_data <- melb_house_data_cleaned[train_indices, ]
test_data <- melb_house_data_cleaned[-train_indices, ]
# 从train_data和test_data中移除PRICE和DISTANCE列
train_data$PRICE <- NULL
train_data$DISTANCE <- NULL
test_data$PRICE <- NULL
test_data$DISTANCE <- NULL
# ==============================================================================
#  决策树模型
model_tree <- rpart(price_interval ~ ., data=train_data, method="class")
print(model_tree)
importance <- model_tree$variable.importance
print(importance)
# 定义交叉验证的控制参数
ctrl <- trainControl(method="cv", number=10)  # 10-fold cross-validation
# 使用caret的train()函数训练模型并进行交叉验证
model_ctrl <- train(price_interval ~ ., data=train_data, method="rpart", trControl=ctrl)
# 查看模型详情
print(model_ctrl)
# 使用最佳cp值训练决策树
best_cp <- 0.01181767
final_tree_model <- rpart(price_interval ~ ., data=train_data, method="class", cp=best_cp)
# 打印训练好的决策树模型的摘要
print(final_tree_model)
# 使用测试数据进行预测
predictions <- predict(final_tree_model, newdata = test_data, type = "class")
# 计算混淆矩阵
confusion_matrix <- confusionMatrix(predictions, test_data$price_interval)
# 输出混淆矩阵
print(confusion_matrix)
# 提取各项性能指标
accuracy <- confusion_matrix$overall["Accuracy"]
precision <- confusion_matrix$byClass["Precision"]
recall <- confusion_matrix$byClass["Recall"]
f1_score <- confusion_matrix$byClass["F1"]
# ==============================================================================
# 设置随机种子以保证结果可重复
set.seed(123)
melb_house_data_cleaned_2 <- melb_house_data_cleaned[ , !(names(melb_house_data_cleaned) %in% c("ADDRESS", "SELLERG", "METHOD", "DATE", "COUNCILARE", "BUILDINGAR", "YEARBUILT", "LATTITUDE", "LONGTITUDE", "PROPERTYCO"))]
melb_house_data_cleaned_2 <- melb_house_data_cleaned_2[melb_house_data_cleaned_2$PRICE >= lower_bound & melb_house_data_cleaned$PRICE <= upper_bound, ]
# 根据新的区间划分price_interval
breaks <- c(1e+05, 2e+05, 3e+05, 4e+05, 5e+05, 6e+05, 7e+05, 8e+05, 9e+05, 1e+06, 1.1e+06, 1.6e+06, 2.1e+06, 2.4e+06)
labels <- c("[1e+05, 2e+05)", "[2e+05, 3e+05)", "[3e+05, 4e+05)", "[4e+05, 5e+05)", "[5e+05, 6e+05)",
"[6e+05, 7e+05)", "[7e+05, 8e+05)", "[8e+05, 9e+05)", "[9e+05, 1e+06)", "[1e+06, 1.1e+06)",
"[1.1e+06, 1.6e+06)", "[1.6e+06, 2.1e+06)", "[2.1e+06, 2.4e+06)")
melb_house_data_cleaned_2$price_interval <- cut(melb_house_data_cleaned_2$PRICE, breaks = breaks, labels = labels, include.lowest = TRUE, right = FALSE)
# 计算训练数据的大小
train_size <- floor(0.7 * nrow(melb_house_data_cleaned_2))
# 随机抽取70%的索引作为训练数据
train_indices <- sample(seq_len(nrow(melb_house_data_cleaned_2)), size = train_size)
# 根据上述索引创建训练数据和测试数据
train_data <- melb_house_data_cleaned_2[train_indices, ]
test_data <- melb_house_data_cleaned_2[-train_indices, ]
# 假设你的因变量是train_data$PriceRange，自变量是train_data的其他列
# ntree表示随机森林中的决策树数量，可以根据需要进行调整
# mtry表示每棵决策树中用于分割的特征数量，也可以根据需要进行调整
rf_model <- randomForest(price_interval ~ ., data = train_data, ntree = 100, mtry = 3)
# 查看随机森林模型的摘要信息
print(rf_model)
str(train_data)
str(test_data)
library(e1071)
# ======== 朴素贝叶斯 ======
# 选择输入特征和目标变量
input_features <- c("ROOMS", "TYPE", "DISTANCE", "POSTCODE", "BEDROOM2", "BATHROOM", "CAR", "LANDSIZE", "REGIONNAME", "AREANUM")
target_variable <- "price_interval"
# 使用naiveBayes函数训练模型
nb_model <- naiveBayes(train_data[, input_features], train_data[, target_variable])
saveRDS(nb_model, "naive_bayes_model.rds")
# 选择测试数据集的输入特征和目标变量
test_input_features <- c("ROOMS", "TYPE", "DISTANCE", "POSTCODE", "BEDROOM2", "BATHROOM", "CAR", "LANDSIZE", "REGIONNAME", "AREANUM")
test_target_variable <- "price_interval"
# 进行预测
predictions <- predict(loaded_model, newdata = test_data[, test_input_features])
# 进行预测
predictions <- predict(nb_model, newdata = test_data[, test_input_features])
# 创建混淆矩阵
confusion_matrix <- table(predictions, test_data[, test_target_variable])
# 计算准确性
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
accuracy
# 导入数据集
melb_house_data <- read.csv("df_selected.csv", header = TRUE)
melb_house_data
str(melb_house_data)
# ================= RANDOM FOREST TREE
# 加载randomForest包
library(randomForest)
# 使用70%的数据作为训练集，30%作为测试集
set.seed(123)  # 设置随机种子以确保结果可重复
sample_size <- floor(0.7 * nrow(melb_house_data))
train_indices <- sample(seq_len(nrow(melb_house_data)), size = sample_size)
train_data <- melb_house_data[train_indices, ]
test_data <- melb_house_data[-train_indices, ]
# 尝试不同的参数组合
n_trees <- c(100, 200, 300)  # Random Forest中树的数量
mtry_values <- c(3, 4, 5)   # 每个分割的特征数量
results <- list()  # 用于存储模型和评估结果
for (ntree in n_trees) {
for (mtry in mtry_values) {
model_name <- paste("RandomForest_ntree", ntree, "mtry", mtry, sep = "_")
# 训练Random Forest模型
rf_model <- randomForest(PRICE_INTERVAL ~ ., data = train_data, ntree = ntree, mtry = mtry)
# 在测试集上进行预测
predictions <- predict(rf_model, test_data)
# 计算模型性能（例如，RMSE、MAE等）
# 这里用RMSE作为示例
rmse <- sqrt(mean((test_data$PRICE_INTERVAL - predictions)^2))
# 将模型和性能结果存储在列表中
results[[model_name]] <- list(model = rf_model, RMSE = rmse)
}
}
# 检查整个数据框中的缺失值
any_na <- any(is.na(melb_house_data))
# 如果 any_na 为 TRUE，则表示数据框中包含 NA 值
if (any_na) {
cat("数据框中包含 NA 值。\n")
# 统计每列（变量）中的 NA 值数量
na_counts <- colSums(is.na(melb_house_data))
print(na_counts)
} else {
cat("数据框中没有 NA 值。\n")
}
melb_house_data$PRICE_INTERVAL
# 创建一个函数，将价格区间字符串转换为代表值
convert_to_numeric <- function(interval) {
# 使用下限值作为代表值
lower_bound <- as.numeric(gsub("[^0-9.-]", "", interval))
return(lower_bound)
}
convert_to_numeric
# 创建一个函数，将价格区间字符串转换为代表值
convert_to_numeric <- function(interval) {
# 使用下限值作为代表值
lower_bound <- as.numeric(gsub("[^0-9.-]", "", interval))
return(lower_bound)
}
# 使用上面的函数将字符串列转换为数值列
melb_house_data$PRICE_NUMERIC <- sapply(melb_house_data$PRICE_INTERVAL, convert_to_numeric)
# 查看新的数据框，包括数值列
head(melb_house_data)
# 导入必备的库
library(rpart)
library(rpart.plot)
library(caret)
library(randomForest)
library(e1071)
# 导入数据集
melb_house_data <- read.csv("df_selected.csv", header = TRUE)
# ================= RANDOM FOREST TREE
str(melb_house_data)
# Get the column name of the non-numeric column
non_numeric_cols <- sapply(melb_house_data, is.character)
# Convert non-numeric columns to factors
melb_house_data[non_numeric_cols] <- lapply(melb_house_data[non_numeric_cols], as.factor)
# Use str() function to verify the result
str(melb_house_data)
# 设置随机数种子以确保结果可重现
set.seed(123)
# 选择特征和目标变量
features <- melb_house_data[, -c("PRICE_INTERVAL")]
set.seed(123)
# 选择特征和目标变量
features <- melb_house_data[, !(colnames(melb_house_data) %in% "PRICE_INTERVAL")]
target <- melb_house_data$PRICE_INTERVAL
# 创建随机森林模型
model1 <- randomForest(target ~ ., data = melb_house_data, ntree = 100, mtry = 3)
# 设置随机数种子以确保结果可重现
set.seed(123)
# 找出数据集中的分类变量
categorical_cols <- sapply(melb_house_data, is.factor)
# 将分类变量独立出来
categorical_data <- melb_house_data[, categorical_cols]
# 使用model.matrix对分类变量进行独热编码
encoded_categorical_data <- model.matrix(~ ., data = categorical_data)
# 合并编码后的分类变量和原始数据集，但不包括目标变量
encoded_melb_house_data <- cbind(melb_house_data[, !categorical_cols], encoded_categorical_data)
# 选择特征和目标变量
features_encoded <- encoded_melb_house_data[, -c("PRICE_INTERVAL")]
# 选择特征和目标变量
features_encoded <- encoded_melb_house_data[, -which(names(encoded_melb_house_data) == "PRICE_INTERVAL")]
target_encoded <- encoded_melb_house_data$PRICE_INTERVAL
# 创建随机森林模型
model1 <- randomForest(target ~ ., data = melb_house_data, ntree = 100, mtry = 3)
# 创建随机森林模型
model1 <- randomForest(target ~ ., data = encoded_melb_house_data , ntree = 100, mtry = 3)
